# 执行py 读取twitter数据，通过kafka-channel存到kafka中
a1.sources = r1
a1.channels = c1

# Search and Replace Interceptor, 看情况考虑是否启用，优化数据
#a1.sources.r1.interceptors = i1
#a1.sources.r1.interceptors.i1.type = search_replace
#a1.sources.r1.interceptors.i1.search-replace.searchPattern = (\\\\r|\\\\n)
#a1.sources.r1.interceptors.i1.search-replace.replaceString = [ ]

# Exec Source
a1.sources.r1.type = exec
a1.sources.r1.restart = false
a1.sources.r1.command = /opt/spark-twitter/twitter-collect/tweetStdout.py /opt/spark-twitter/twitter-collect/myLog.log
a1.sources.r1.channels = c1


# kafka channel
a1.channels.c1.type = org.apache.flume.channel.kafka.KafkaChannel
a1.channels.c1.kafka.bootstrap.servers = gpu7:9092
a1.channels.c1.kafka.topic = alex1
a1.channels.c1.kafka.consumer.group.id = flume-consumer
#Flume的headers中的信息混合着内容一起写入kafka的消息中，无论headers是否为空时都出现了奇怪的特殊字符--暂未解决
a1.channels.c1.kafka.parseAsFlumeEvent=false